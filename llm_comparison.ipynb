{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPdI4d//svbdHlSsUqvici",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yogesh914/emogo-ml/blob/main/llm_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma ü¶ã vs Mistral üèµÔ∏è vs Llama ü¶ô"
      ],
      "metadata": {
        "id": "46nbjUHHoqPG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTWCUVbZnzgL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers pydub accelerate bitsandbytes hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "vwJtau_xn852"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from IPython.display import Markdown, display\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from moviepy.editor import VideoFileClip\n",
        "import numpy as np\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AE6MTwCvn_yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "YwEfUj4iojau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "model_inputs = encodeds.to(device)\n",
        "model.to(device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])\n"
      ],
      "metadata": {
        "id": "5LY-Zu2QoQ2U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}